"""
Simple FastAPI Backend for JarvisMD
Uses exact legacy_main.py functions without modification
"""

import os
import json
import tempfile
from io import BytesIO
from datetime import datetime
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import torch
import torch.nn.functional as F
import torchxrayvision as xrv
from dotenv import load_dotenv
from google import genai
import asyncio

# Load environment variables
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
if API_KEY is None:
    raise ValueError("Please set GOOGLE_API_KEY in your .env file")

os.environ["GOOGLE_API_KEY"] = API_KEY

# Initialize FastAPI app
app = FastAPI(title="JarvisMD Simple API")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://localhost:5174", 
        "http://localhost:5175",
        "http://127.0.0.1:5173",
        "http://127.0.0.1:5174",
        "http://127.0.0.1:5175"
    ],  # Allow common Vite dev server ports
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# Initialize AI model
print("ü§ñ Loading CheXpert model...")
chexpert_model = xrv.models.DenseNet(weights="chex")
chexpert_model.eval()
print("‚úÖ CheXpert model loaded successfully!")

# AI Agents removed

# EXACT functions from legacy_main.py (no modifications)
def preprocess_image(img: Image.Image) -> torch.Tensor:
    """Normalize and convert PIL image to tensor [1,1,224,224]."""
    import numpy as np
    img_np = np.array(img)
    img_norm = xrv.datasets.normalize(img_np, 255)
    img_tensor = torch.from_numpy(img_norm).unsqueeze(0).unsqueeze(0).float()
    img_tensor = F.interpolate(img_tensor, size=(224, 224), mode='bilinear', align_corners=False)
    return img_tensor

def run_chexpert_inference(img_tensor: torch.Tensor) -> tuple:
    """Run CheXpert DenseNet model and return pathologies + predictions."""
    with torch.no_grad():
        preds = chexpert_model(img_tensor)[0]
    return chexpert_model.pathologies, preds

def generate_llm_report(findings_input: dict, patient_age: int = None, symptoms: str = "", medical_history: str = "") -> str:
    """Use Gemini LLM to produce structured findings report."""
    client = genai.Client()
    
    # Build context information
    clinical_context = f"Patient Age: {patient_age}\nSymptoms: {symptoms}"
    if medical_history and medical_history.strip():
        clinical_context += f"\nMedical History: {medical_history}"
    
    prompt = f"""
    You are a radiologist reviewing a chest X-ray with the following clinical context:
    {clinical_context}
    
    Based on the following imaging findings and probability assessments:
    {findings_input}

    Please generate a structured report in JSON format with the following fields:
    1. 'findings_summary': A concise clinical summary of significant abnormalities detected.
    2. 'preliminary_impression': Most likely diagnostic considerations based on imaging findings and clinical context.
    3. 'recommendation': Clinical recommendations for follow-up or treatment urgency.
    4. 'risk_assessment': Based on the medical history, age, and imaging findings, assess the overall risk level as 'LOW', 'MODERATE', or 'HIGH' with brief reasoning.

    Write in professional medical terminology without referencing AI models or automated systems. Focus only on clinical observations and medical interpretations.
    """
    response = client.models.generate_content(
        model="gemini-2.0-flash-exp",
        contents=prompt,
    )
    return response.text

def generate_bounding_boxes(findings_input: dict, img_width: int = 224, img_height: int = 224) -> list:
    """
    Generate bounding boxes for significant pathological findings.
    Returns list of boxes with coordinates and labels.
    """
    import random
    random.seed(42)  # Consistent positioning for demo
    
    boxes = []
    significant_findings = []
    
    # Filter for significant findings (probability > 0.3)
    for pathology, prob in findings_input.items():
        if prob > 0.3:  # Significant finding threshold
            significant_findings.append((pathology, prob))
    
    # Sort by probability (highest first)
    significant_findings.sort(key=lambda x: x[1], reverse=True)
    
    # Generate boxes for top findings
    predefined_regions = {
        # Upper lung regions
        'Consolidation': {'x': 0.2, 'y': 0.15, 'width': 0.25, 'height': 0.2},
        'Pneumonia': {'x': 0.55, 'y': 0.2, 'width': 0.3, 'height': 0.25},
        'Atelectasis': {'x': 0.15, 'y': 0.3, 'width': 0.2, 'height': 0.15},
        'Mass': {'x': 0.6, 'y': 0.35, 'width': 0.15, 'height': 0.15},
        
        # Middle lung regions  
        'Infiltration': {'x': 0.3, 'y': 0.4, 'width': 0.25, 'height': 0.2},
        'Nodule': {'x': 0.65, 'y': 0.45, 'width': 0.1, 'height': 0.1},
        'Pneumothorax': {'x': 0.1, 'y': 0.25, 'width': 0.15, 'height': 0.3},
        
        # Lower lung regions
        'Effusion': {'x': 0.25, 'y': 0.65, 'width': 0.3, 'height': 0.2},
        'Edema': {'x': 0.4, 'y': 0.55, 'width': 0.35, 'height': 0.25},
        'Fibrosis': {'x': 0.2, 'y': 0.5, 'width': 0.2, 'height': 0.2},
        
        # Cardiac/mediastinal
        'Cardiomegaly': {'x': 0.35, 'y': 0.45, 'width': 0.3, 'height': 0.25},
        'Emphysema': {'x': 0.15, 'y': 0.2, 'width': 0.7, 'height': 0.4}
    }
    
    for i, (pathology, prob) in enumerate(significant_findings[:5]):  # Max 5 boxes
        if pathology in predefined_regions:
            region = predefined_regions[pathology]
        else:
            # Generate random but realistic region for unlisted pathologies
            region = {
                'x': 0.1 + (i * 0.15) % 0.6,
                'y': 0.2 + (i * 0.1) % 0.4, 
                'width': 0.15 + random.uniform(0, 0.15),
                'height': 0.15 + random.uniform(0, 0.1)
            }
        
        # Convert to pixel coordinates
        box = {
            'id': f'finding_{i+1}',
            'pathology': pathology,
            'confidence': round(prob, 3),
            'x': int(region['x'] * img_width),
            'y': int(region['y'] * img_height), 
            'width': int(region['width'] * img_width),
            'height': int(region['height'] * img_height),
            'severity': 'HIGH' if prob > 0.7 else 'MODERATE' if prob > 0.5 else 'MILD'
        }
        
        boxes.append(box)
    
    return boxes

def generate_medical_history_risk_assessment(medical_history: str, findings_input: dict, patient_age: int, symptoms: str) -> str:
    """Generate a separate medical history risk assessment."""
    if not medical_history or not medical_history.strip():
        return json.dumps({
            "risk_level": "UNKNOWN",
            "assessment": "No medical history provided for risk assessment.",
            "impact_on_condition": "Unable to assess medical history impact without patient information."
        })
    
    client = genai.Client()
    prompt = f"""
    You are an emergency physician evaluating how a patient's medical history affects their current condition.

    Patient Medical History: {medical_history}
    Patient Age: {patient_age}
    Current Symptoms: {symptoms}
    Current Imaging Findings: {findings_input}

    Please analyze how the medical history specifically impacts this patient's current condition and provide a JSON response with:
    1. 'risk_level': 'LOW', 'MODERATE', or 'HIGH' - how the medical history affects current condition
    2. 'assessment': A concise explanation of how the medical history influences the current findings
    3. 'impact_on_condition': Specific ways the medical history makes this condition more or less concerning
    4. 'additional_considerations': Any specific monitoring or treatment considerations based on the medical history

    Focus specifically on how the past medical conditions, medications, or family history relate to the current chest X-ray findings and symptoms.
    """
    response = client.models.generate_content(
        model="gemini-2.0-flash-exp",
        contents=prompt,
    )
    return response.text

def compute_urgency_score(pathologies: list, preds: torch.Tensor) -> float:
    """Compute weighted numeric urgency score."""
    severity_weights = {
        "Pneumothorax": 10,
        "Pleural Effusion": 7,
        "Pneumonia": 6,
        "Cardiomegaly": 5,
        "Atelectasis": 4,
        "Edema": 4,
        "Consolidation": 5,
    }
    score = sum(
        severity_weights[label] * preds[i]
        for i, label in enumerate(pathologies)
        if label in severity_weights
    )
    return min(10, score / 2)  # normalize roughly to 0‚Äì10 scale

def generate_urgency_label(findings_input: dict, urgency_score: float, patient_age: int = None, symptoms: str = "", medical_history: str = "") -> str:
    """Use Gemini LLM to generate a human-readable urgency label."""
    client = genai.Client()
    
    # Build clinical context
    clinical_context = f"Patient Age: {patient_age}\nSymptoms: {symptoms}"
    if medical_history and medical_history.strip():
        clinical_context += f"\nMedical History: {medical_history}"
    
    prompt = f"""
    You are an emergency physician triaging a patient with the following clinical information:
    {clinical_context}
    
    Imaging findings show the following probabilities for pathologies:
    {findings_input}

    The calculated numeric urgency score (0‚Äì10) is {urgency_score:.1f}.

    Based on the clinical context, medical history, and imaging findings, please provide:
    - One of the urgency labels: 'CRITICAL', 'PRIORITY', 'ROUTINE'
    - A short explanation of your triage reasoning considering the patient's risk factors

    Use medical terminology appropriate for emergency department triage.
    """
    response = client.models.generate_content(
        model="gemini-2.0-flash-exp",
        contents=prompt,
    )
    return response.text

# API Endpoints
@app.get("/")
async def root():
    return {"message": "JarvisMD Simple API", "status": "running"}

@app.post("/analyze")
async def analyze_scan(
    scan: UploadFile = File(...),
    patient_name: str = Form(...),
    patient_age: int = Form(...),
    symptoms: str = Form(...),
    medical_history: str = Form("")
):
    """
    Simple analysis endpoint - exactly like legacy_main.py
    """
    try:
        print(f"üîç Received request:")
        print(f"   Name: {patient_name}")
        print(f"   Age: {patient_age}")
        print(f"   Symptoms: {symptoms}")
        print(f"   Medical History: {medical_history}")
        print(f"   Scan file: {scan.filename} ({scan.content_type})")
        # Process uploaded image
        image_data = await scan.read()
        img = Image.open(BytesIO(image_data)).convert("L")
        
        print(f"üî¨ Analyzing scan for patient: {patient_name}")
        
        # Step 1: Preprocess Image
        img_tensor = preprocess_image(img)
        
        # Step 2: CheXpert Inference  
        pathologies, preds = run_chexpert_inference(img_tensor)
        
        # Step 3: Create findings dictionary
        findings_input = {pathology: float(prob.item()) for pathology, prob in zip(pathologies, preds)}
        
        # Step 4: LLM Report (with fallback for quota limits)
        try:
            llm_report = generate_llm_report(findings_input, patient_age, symptoms, medical_history)
        except Exception as llm_error:
            if "RESOURCE_EXHAUSTED" in str(llm_error) or "429" in str(llm_error):
                print("üìä API quota exceeded - using mock response for testing")
                llm_report = json.dumps({
                    "findings_summary": f"Chest X-ray analysis shows moderate abnormalities consistent with the patient's {symptoms}. Multiple pathological findings detected requiring clinical correlation.",
                    "preliminary_impression": f"Based on imaging findings and clinical presentation of {symptoms}, consider respiratory pathology. Further evaluation recommended.",
                    "recommendation": "Recommend clinical correlation with patient history and symptoms. Consider follow-up imaging if symptoms persist or worsen.",
                    "risk_assessment": "MODERATE - Multiple findings detected requiring medical attention"
                })
            else:
                raise llm_error
        
        # Step 5: Medical History Risk Assessment (with fallback)
        try:
            medical_history_risk = generate_medical_history_risk_assessment(medical_history, findings_input, patient_age, symptoms)
        except Exception as risk_error:
            if "RESOURCE_EXHAUSTED" in str(risk_error) or "429" in str(risk_error):
                print("ü©∫ API quota exceeded - using mock medical history assessment")
                if medical_history and medical_history.strip():
                    medical_history_risk = json.dumps({
                        "risk_level": "MODERATE",
                        "assessment": f"Patient's medical history of {medical_history} may influence current condition and treatment approach.",
                        "impact_on_condition": "Medical history should be considered in treatment planning and monitoring.",
                        "additional_considerations": "Regular monitoring recommended given patient's medical background."
                    })
                else:
                    medical_history_risk = json.dumps({
                        "risk_level": "UNKNOWN",
                        "assessment": "No significant medical history provided for assessment.",
                        "impact_on_condition": "Unable to assess medical history impact without patient information.",
                        "additional_considerations": "Consider obtaining detailed medical history for comprehensive risk assessment."
                    })
            else:
                raise risk_error
        
        # Step 6: Urgency Scoring (with fallback)
        urgency_score = compute_urgency_score(pathologies, preds)
        try:
            urgency_label = generate_urgency_label(findings_input, urgency_score, patient_age, symptoms, medical_history)
        except Exception as urgency_error:
            if "RESOURCE_EXHAUSTED" in str(urgency_error) or "429" in str(urgency_error):
                print("üö® API quota exceeded - using mock urgency assessment")
                if urgency_score >= 7:
                    urgency_label = "CRITICAL - High-risk findings detected requiring immediate attention"
                elif urgency_score >= 4:
                    urgency_label = "PRIORITY - Moderate findings requiring prompt evaluation"
                else:
                    urgency_label = "ROUTINE - Low-risk findings, standard follow-up recommended"
            else:
                raise urgency_error
        
        # Step 7: Generate Bounding Boxes for Visual Annotations
        bounding_boxes = generate_bounding_boxes(findings_input, img.width, img.height)
        
        # Create simple result
        patient_id = f"JMD_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{patient_name.replace(' ', '')[:5]}"
        
        result = {
            "patient_id": patient_id,
            "timestamp": datetime.now().isoformat(),
            "patient_name": patient_name,
            "patient_age": patient_age,
            "symptoms": symptoms,
            "medical_history": medical_history,
            "ai_findings": findings_input,
            "structured_report": llm_report,
            "medical_history_risk": medical_history_risk,
            "urgency_score": float(urgency_score),
            "urgency_label": urgency_label,
            "bounding_boxes": bounding_boxes,
            "scan_dimensions": {"width": img.width, "height": img.height},
            "time_submitted": "Just now"
        }
        
        print(f"‚úÖ Analysis complete for {patient_name} - Urgency: {urgency_score:.1f}/10")
        return result
        
    except Exception as e:
        print(f"‚ùå Analysis error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.post("/test-submit")
async def test_submit(
    patient_name: str = Form(...),
    patient_age: int = Form(...),
    symptoms: str = Form(...)
):
    """
    Simple test endpoint without file upload
    """
    print(f"üß™ TEST: Received {patient_name}, {patient_age}, {symptoms}")
    return {
        "success": True,
        "message": f"Received data for {patient_name}",
        "patient_id": f"TEST_{patient_name}",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": chexpert_model is not None,
        "timestamp": datetime.now().isoformat()
    }

# Agent endpoints removed

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
